% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/metapredict_confusion.R
\name{calcConfusionValidation}
\alias{calcConfusionValidation}
\title{Calculate confusion matrices (or matrix) for validation datasets.}
\usage{
calcConfusionValidation(
  predsList,
  lambda,
  sampleMetadata,
  className = "class",
  classLevels = NA,
  each = TRUE
)
}
\arguments{
\item{predsList}{list of predictions from \code{\link[=metapredict]{metapredict()}}.}

\item{lambda}{value of lambda at which to use predictions.}

\item{sampleMetadata}{data.frame of sample metadata.}

\item{className}{name of column in \code{sampleMetadata} containing the true
labels.}

\item{classLevels}{Order of classes in the confusion matrix. If \code{NA}
(default), then the function uses the order in \code{cvFit}.}

\item{each}{logical indicating whether to calculate a confusion matrix for
each validation dataset (default) or one confusion matrix including all
datasets.}
}
\value{
If \code{isTRUE(each)}, a list of objects of class \code{table}. Otherwise, an
object of class \code{table}.
}
\description{
Calculate confusion matrices based on predictions for validation datasets.
}
